### 第十一与十二周工作总结

左晨阳 2022010896

#### 两周工作总结

- 为 UPID 设置了 Linux 和 Nimbos 共享的内存区域，并在 Linux 和 Nimbos 中实现了简易的专用分配器。
- 与冉博涵同学实现的 shadow 进程机制对接，完成代码合并。
- 尝试了纯用户态 Syscall Forwarding，能够部分转发并完成 write 系统调用，但仍然存在错误

下周工作计划：

- 修复跨域 uintr 存在不稳定的问题。
- 实现基于 uintr 的 Syscall Forwarding，确定是否应采取纯用户态的方式。

#### 工作记录

**UPID 共享区域**

为了避免 Linux 和 Nimbos 两个域互相完全暴露内存，修改之前对跨域 uintr 的实现，在特定的内存区域中分配 UPID。之前为了通过 RVM 1.5 启动 RTOS，已经在 Linux 启动参数中划定了保留区域 0x3a000000 - 0x4a000000。其中 0x3a000000 - 0x42000000 给 hypervisor 使用。现额外将 0x41ffe000 - 0x42000000 作为 UPID 的共享区域，其中 0x41ffe000 - 0x41fff000 作为 Linux 可写区域，0x41fff000 - 0x42000000 作为 Nimbos 可写区域。

需要修改 jailhouse 对 hv_region 的定义：

```c
#define HV_PHYS_START 0x3a000000
#define HV_MEM_SIZE ((128 << 20) - 0x2000) // 128M
#define RT_MEM_SIZE (128 << 20) // 128M

static const struct jailhouse_enable_args enable_args = {
	.hv_region =
		{
			.start = HV_PHYS_START,
			.size = HV_MEM_SIZE,
		},
	.rt_region = {
		.start = HV_PHYS_START + HV_MEM_SIZE + 0x2000,
		.size = RT_MEM_SIZE,
	}
};
```

RVM 1.5 不需要修改。

**专用内存分配器**

在 nimbos 和 Linux 中分别实现了简易的专用内存分配器，分配时直接线性查找空闲内存块，未来可以考虑使用更高效的分配算法。nimbos 中，在内核初始映射中添加：

```rust
ms.insert(MapArea::new_offset(
    VirtAddr::new(UPID_SHARE_MEM_VIRT_START), 
    PhysAddr::new(UPID_SHARE_MEM_PHYS_START), 
    UPID_SHARE_MEM_SIZE, 
    MemFlags::READ | MemFlags::WRITE
));
```

目前实现中保留了在堆上分配内存的能力，当 rvm feature 启用时，会使用专用分配器，否则使用堆分配。

```rust
#[cfg(feature = "rvm")]
{
    ctx.uintr_upid_ctx = Some(
        Box::new(UintrUpidCtx {
            upid: UintrBox::new().unwrap(),
        })
    );
}
#[cfg(not(feature = "rvm"))]
{
    ctx.uintr_upid_ctx = Some(Box::new(UintrUpidCtx {
        // uvec_mask: 0,
        upid: Box::new(UintrUpid {
            nc: UintrNc {
                status: 0,
                reserved1: 0,
                nv: 0,
                reserved2: 0,
                ndst: 0,
            },
            puir: 0,
        }),
        // receiver_active: true,
    }));
}
```

Linux 中，也需要在启动时增加内存映射：

```c
void upid_shared_mem_init(void) {
	static struct resource *upid_mem_res;

	upid_mem_res = request_mem_region(UPID_SHARED_MEM_PHYS_ADDR, UPID_SHARED_MEM_SIZE, "UPID shared mem");
	if (!upid_mem_res) {
		pr_err("request_mem_region failed for upid memory.\n");
		return;
	}

	upid_shared_mem_start = ioremap(UPID_SHARED_MEM_PHYS_ADDR, UPID_SHARED_MEM_SIZE);
    if (!upid_shared_mem_start) {
		// vunmap(vma->addr);
		release_mem_region(upid_mem_res->start, resource_size(upid_mem_res));
        pr_err("Failed to map 0x%llx\n", UPID_SHARED_MEM_PHYS_ADDR);
    }
	else {
		pr_info("Mapped phys 0x%llx -> virt %px\n", UPID_SHARED_MEM_PHYS_ADDR, upid_shared_mem_start);
		upid_initialized = true;
	}
}
```

这里仿照 jailhouse 的写法，不太确定是否正确。如果映射成功，将使用专用分配器分配内存，否则回退到堆分配。

**工作合并**

合并后，程序运行基本符合预期，uintr 机制和 shadow 进程机制都能正常工作。目前，使用不同 irq_num 实现不同的进程区分，容易发生冲突，考虑改进。

**Syscall Forwarding**

尝试了纯用户态的 Syscall Forwarding，能够部分转发并完成 write 系统调用，但仍然存在错误。具体而言，第一次调用 sys_write 时，初始化 cross_uintr 机制，与 shadow 程序交换 UPID，同时向 shadow 程序发送一个 scf_descriptor 结构的地址。由于 shadow 机制实现了内存映射的复制，因此 shadow 程序可以直接访问这个地址。初始化完成后，每次调用 sys_write 时，都会先填写 scf_descriptor 结构体，然后通过 uintr 通知 shadow 程序。shadow 程序收到 uintr 后，读取 scf_descriptor 结构体，完成 write 系统调用。

问题：

1. 目前，没有实现 syscall buffer，多线程情况下会阻塞。
2. 出现 segmentation fault，尚不清楚原因，推测与 handler 内的非安全操作有关。
3. fork 的处理未测试。
4. 速度很慢，推测和 qemu 的实现有关。
5. 目前，调用返回仍然使用了 uintr，这可能是不必要的，直接在 descriptor 中原子化修改完成标记即可。
6. 这样的实现是否有意义？是否最好使用内核态的方式？
7. 其他线程安全和一致性问题。
8. 观察到发多次 uintr 可能只能收到一次。
